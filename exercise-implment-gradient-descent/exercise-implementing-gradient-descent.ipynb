{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5eae09e-b55d-4a95-9c0c-b26331080475",
   "metadata": {},
   "source": [
    "# Exercise: Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fdcb33-2a52-416e-975a-6095d8ea5425",
   "metadata": {},
   "source": [
    "Implementing gradient descent\n",
    "Okay, now we know how to update our weights:\n",
    "\n",
    "You've seen how to implement that for a single update, but how do we translate that code to calculate many weight updates so our network will learn?\n",
    "\n",
    "As an example, I'm going to have you use gradient descent to train a network on graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv). This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "The goal here is to predict if a student will be admitted to a graduate program based on these features. For this, we'll use a network with one output layer with one unit. We'll use a sigmoid function for the output unit activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b1f20-ed57-418a-866c-c77e53bebe0a",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "### Data cleanup\r\n",
    "You might think there will be three input units, but we actually need to transform the data first.\r\n",
    "\r\n",
    "The rank feature is categorical, the numbers don't encode any sort of relative values.\r\n",
    "Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2.\r\n",
    "Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros.\r\n",
    "Rows with rank 1 have one in the rank 1 dummy column, and zeros in all other columns.\r\n",
    "Rows with rank 2 have one in the rank 2 dummy column, and zeros in all other columns. Andexercise below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d6c22-7ac1-41a2-ab89-78d4b38dfb59",
   "metadata": {},
   "source": [
    "### Standardizing the GRE and GPA Data\n",
    "We'll also need to standardize the GRE and GPA data, which means scaling the values such that they have zero mean and a standard deviation of 1.\n",
    "\n",
    "- This is necessary because the sigmoid function squashes really small and really large inputs.\n",
    "- The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too.\n",
    "- Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train.\n",
    "- Instead, if we standardize the data, we can initialize the weights easily and everyone is happy.\n",
    "\n",
    "This is just a brief run-through, you'll learn more about preparing data later. If you're interested in how I did this, check out the data_prep.py file in the programming exercise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd566b93-ddd5-43c4-8cc9-4990f2d1ae61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit  gre   gpa  rank\n",
       "0        0  380  3.61     3\n",
       "1        1  660  3.67     3\n",
       "2        1  800  4.00     1\n",
       "3        1  640  3.19     4\n",
       "4        0  520  2.93     4\n",
       "..     ...  ...   ...   ...\n",
       "395      0  620  4.00     2\n",
       "396      0  560  3.04     3\n",
       "397      0  460  2.63     2\n",
       "398      0  700  3.65     2\n",
       "399      0  600  3.89     3\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc414fb-d8d2-4f9b-b7ee-c0ccd54b5503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit  gre   gpa  rank_1  rank_2  rank_3  rank_4\n",
       "0        0  380  3.61       0       0       1       0\n",
       "1        1  660  3.67       0       0       1       0\n",
       "2        1  800  4.00       1       0       0       0\n",
       "3        1  640  3.19       0       0       0       1\n",
       "4        0  520  2.93       0       0       0       1\n",
       "..     ...  ...   ...     ...     ...     ...     ...\n",
       "395      0  620  4.00       0       1       0       0\n",
       "396      0  560  3.04       0       0       1       0\n",
       "397      0  460  2.63       0       1       0       0\n",
       "398      0  700  3.65       0       1       0       0\n",
       "399      0  600  3.89       0       0       1       0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make dummy variables for rank\n",
    "data = pd.get_dummies(admissions, prefix='rank', columns=['rank'], dtype=int)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0888a41-64d3-4dfc-aebd-4b474f8dd7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.798011</td>\n",
       "      <td>0.578348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625884</td>\n",
       "      <td>0.736008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452749</td>\n",
       "      <td>-0.525269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.586063</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0.279614</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.239793</td>\n",
       "      <td>-0.919418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.105469</td>\n",
       "      <td>-1.996759</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>0.972155</td>\n",
       "      <td>0.683454</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>0.106478</td>\n",
       "      <td>1.314093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit       gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "0        0 -1.798011  0.578348       0       0       1       0\n",
       "1        1  0.625884  0.736008       0       0       1       0\n",
       "2        1  1.837832  1.603135       1       0       0       0\n",
       "3        1  0.452749 -0.525269       0       0       0       1\n",
       "4        0 -0.586063 -1.208461       0       0       0       1\n",
       "..     ...       ...       ...     ...     ...     ...     ...\n",
       "395      0  0.279614  1.603135       0       1       0       0\n",
       "396      0 -0.239793 -0.919418       0       0       1       0\n",
       "397      0 -1.105469 -1.996759       0       1       0       0\n",
       "398      0  0.972155  0.683454       0       1       0       0\n",
       "399      0  0.106478  1.314093       0       0       1       0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d9838d-5480-4a28-b0e9-826a442ee2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.loc[sample], data.drop(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2197c0-d394-42f8-af2e-099a1991add0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>0.289305</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.625884</td>\n",
       "      <td>1.445476</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1.318426</td>\n",
       "      <td>-0.131120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.066657</td>\n",
       "      <td>-1.208461</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.279614</td>\n",
       "      <td>0.946220</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.239793</td>\n",
       "      <td>-1.155908</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-0.412928</td>\n",
       "      <td>-0.577822</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.625884</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-0.412928</td>\n",
       "      <td>-0.288780</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gre       gpa  rank_1  rank_2  rank_3  rank_4\n",
       "209 -0.066657  0.289305       0       1       0       0\n",
       "280  0.625884  1.445476       0       1       0       0\n",
       "33   1.837832  1.603135       0       0       1       0\n",
       "210  1.318426 -0.131120       0       0       0       1\n",
       "93  -0.066657 -1.208461       0       1       0       0\n",
       "..        ...       ...     ...     ...     ...     ...\n",
       "393  0.279614  0.946220       0       1       0       0\n",
       "134 -0.239793 -1.155908       0       1       0       0\n",
       "306 -0.412928 -0.577822       1       0       0       0\n",
       "383  0.625884  1.603135       1       0       0       0\n",
       "319 -0.412928 -0.288780       1       0       0       0\n",
       "\n",
       "[360 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0924b750-f63a-4cf9-aea2-54f423700876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209    0\n",
       "280    0\n",
       "33     1\n",
       "210    0\n",
       "93     0\n",
       "      ..\n",
       "393    1\n",
       "134    0\n",
       "306    1\n",
       "383    0\n",
       "319    0\n",
       "Name: admit, Length: 360, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c61ea-8e45-4e89-8205-aedbfde97ea1",
   "metadata": {},
   "source": [
    "Now that the data is ready, we see that there are six input features: gre, gpa, and the four rank dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d5976-9ebb-453a-a3b4-e8e64621b0ba",
   "metadata": {},
   "source": [
    "## Mean Square Error\n",
    "We're going to make a small change to how we calculate the error here. Instead of the SSE, we're going to use the mean of the square errors (MSE). Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, \n",
    "�\n",
    "m to take the average. This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a3f17-22e8-4684-ba57-0b1ced62e03d",
   "metadata": {},
   "source": [
    "## Implementing with NumPy\r\n",
    "For the most part, this is pretty straightforward with NumPy.\r\n",
    "\r\n",
    "First, you'll need to initialize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is \r\n",
    "1\r\n",
    "/\r\n",
    "�\r\n",
    "1/ \r\n",
    "n\r\n",
    "​\r\n",
    "  where \r\n",
    "�\r\n",
    "n is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2eea923-58e6-4738-89ff-63509d05bd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_prep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_prep\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m features, targets, features_test, targets_test\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    Calculate sigmoid\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_prep'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def update_weights(weights, features, targets, learnrate):\n",
    "    \"\"\"\n",
    "    Complete a single epoch of gradient descent and return updated weights\n",
    "    \"\"\"\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    # Loop through all records, x is the input, y is the target\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # TODO: Calculate the output of f(h) by passing h (the dot product\n",
    "        # of x and weights) into the activation function (sigmoid).\n",
    "        # Replace None with appropriate code\n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # TODO: Calculate the error by subtracting the network output\n",
    "        # from the target (y).\n",
    "        # Replace None with appropriate code\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term by multiplying the error by the\n",
    "        # gradient. Recall that the gradient of the sigmoid f(h) is\n",
    "        # f(h)*(1−f(h)) so you do not need to call any additional\n",
    "        # functions and can simply apply this formula to the output and\n",
    "        # error you already calculated.\n",
    "        # Replace None with appropriate code\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # TODO: Update the weight step by multiplying the error term by\n",
    "        # the input (x) and adding this to the current weight step.\n",
    "        # Replace 0 with appropriate code\n",
    "        del_w += error_term * x\n",
    "\n",
    "    n_records = features.shape[0]\n",
    "    # TODO: Update the weights by adding the learning rate times the\n",
    "    # change in weights divided by the number of records.\n",
    "    # Replace 0 with appropriate code\n",
    "    weights += learnrate * del_w / n_records\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def gradient_descent(features, targets, epochs=1000, learnrate=0.5):\n",
    "    \"\"\"\n",
    "    Perform the complete gradient descent process on a given dataset\n",
    "    \"\"\"\n",
    "    # Use to same seed to make debugging easier\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize loss and weights\n",
    "    last_loss = None\n",
    "    n_features = features.shape[1]\n",
    "    weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "    # Repeatedly update the weights based on the number of epochs\n",
    "    for e in range(epochs):\n",
    "        weights = update_weights(weights, features, targets, learnrate)\n",
    "\n",
    "        # Printing out the MSE on the training set every 10 epochs.\n",
    "        # Initially this will print the same loss every time. When all of\n",
    "        # the TODOs are complete, the MSE should decrease with each\n",
    "        # printout\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(features, weights))\n",
    "            loss = np.mean((out - targets) ** 2)\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            \n",
    "    return weights\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "weights = gradient_descent(features, targets)\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd514fa-c6f1-4e54-b1f4-8c80fbb67b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807f74a-1647-4dab-9ec5-9fd39160bfce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
